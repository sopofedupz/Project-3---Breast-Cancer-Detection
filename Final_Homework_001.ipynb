{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/Resources'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Resources/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= data['diagnosis'].map({'M':0,'B':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "564    0\n",
       "565    0\n",
       "566    0\n",
       "567    0\n",
       "568    1\n",
       "Name: diagnosis, Length: 569, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= data.drop(columns=['id','diagnosis','Unnamed: 32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.33)\n",
    "N ,D =X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalling the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now creating the model\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Input(shape=(D,)),\n",
    "                                    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can avoid this error by converting your labels to arrays before calling model.fit()\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "Y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/100\n",
      "381/381 [==============================] - 1s 2ms/sample - loss: 1.1170 - accuracy: 0.2231 - val_loss: 1.0563 - val_accuracy: 0.2181\n",
      "Epoch 2/100\n",
      "381/381 [==============================] - 0s 200us/sample - loss: 1.0146 - accuracy: 0.2572 - val_loss: 0.9553 - val_accuracy: 0.2766\n",
      "Epoch 3/100\n",
      "381/381 [==============================] - 0s 159us/sample - loss: 0.9175 - accuracy: 0.2966 - val_loss: 0.8679 - val_accuracy: 0.3245\n",
      "Epoch 4/100\n",
      "381/381 [==============================] - 0s 157us/sample - loss: 0.8355 - accuracy: 0.3911 - val_loss: 0.7888 - val_accuracy: 0.4362\n",
      "Epoch 5/100\n",
      "381/381 [==============================] - 0s 183us/sample - loss: 0.7621 - accuracy: 0.4777 - val_loss: 0.7201 - val_accuracy: 0.5266\n",
      "Epoch 6/100\n",
      "381/381 [==============================] - 0s 155us/sample - loss: 0.6973 - accuracy: 0.5669 - val_loss: 0.6623 - val_accuracy: 0.5851\n",
      "Epoch 7/100\n",
      "381/381 [==============================] - 0s 157us/sample - loss: 0.6420 - accuracy: 0.6168 - val_loss: 0.6131 - val_accuracy: 0.6543\n",
      "Epoch 8/100\n",
      "381/381 [==============================] - 0s 145us/sample - loss: 0.5968 - accuracy: 0.6903 - val_loss: 0.5691 - val_accuracy: 0.7447\n",
      "Epoch 9/100\n",
      "381/381 [==============================] - 0s 157us/sample - loss: 0.5554 - accuracy: 0.7454 - val_loss: 0.5322 - val_accuracy: 0.7766\n",
      "Epoch 10/100\n",
      "381/381 [==============================] - 0s 144us/sample - loss: 0.5210 - accuracy: 0.7717 - val_loss: 0.4994 - val_accuracy: 0.8032\n",
      "Epoch 11/100\n",
      "381/381 [==============================] - 0s 158us/sample - loss: 0.4896 - accuracy: 0.7927 - val_loss: 0.4713 - val_accuracy: 0.8191\n",
      "Epoch 12/100\n",
      "381/381 [==============================] - 0s 144us/sample - loss: 0.4629 - accuracy: 0.8241 - val_loss: 0.4462 - val_accuracy: 0.8351\n",
      "Epoch 13/100\n",
      "381/381 [==============================] - 0s 160us/sample - loss: 0.4391 - accuracy: 0.8451 - val_loss: 0.4237 - val_accuracy: 0.8617\n",
      "Epoch 14/100\n",
      "381/381 [==============================] - 0s 149us/sample - loss: 0.4173 - accuracy: 0.8635 - val_loss: 0.4041 - val_accuracy: 0.8723\n",
      "Epoch 15/100\n",
      "381/381 [==============================] - 0s 160us/sample - loss: 0.3982 - accuracy: 0.8740 - val_loss: 0.3863 - val_accuracy: 0.8883\n",
      "Epoch 16/100\n",
      "381/381 [==============================] - 0s 168us/sample - loss: 0.3811 - accuracy: 0.8793 - val_loss: 0.3699 - val_accuracy: 0.8936\n",
      "Epoch 17/100\n",
      "381/381 [==============================] - 0s 143us/sample - loss: 0.3649 - accuracy: 0.8950 - val_loss: 0.3555 - val_accuracy: 0.8936\n",
      "Epoch 18/100\n",
      "381/381 [==============================] - 0s 159us/sample - loss: 0.3507 - accuracy: 0.8976 - val_loss: 0.3422 - val_accuracy: 0.8989\n",
      "Epoch 19/100\n",
      "381/381 [==============================] - 0s 148us/sample - loss: 0.3377 - accuracy: 0.9029 - val_loss: 0.3299 - val_accuracy: 0.9043\n",
      "Epoch 20/100\n",
      "381/381 [==============================] - 0s 163us/sample - loss: 0.3252 - accuracy: 0.9081 - val_loss: 0.3188 - val_accuracy: 0.9043\n",
      "Epoch 21/100\n",
      "381/381 [==============================] - 0s 160us/sample - loss: 0.3138 - accuracy: 0.9108 - val_loss: 0.3087 - val_accuracy: 0.9149\n",
      "Epoch 22/100\n",
      "381/381 [==============================] - 0s 144us/sample - loss: 0.3037 - accuracy: 0.9160 - val_loss: 0.2991 - val_accuracy: 0.9202\n",
      "Epoch 23/100\n",
      "381/381 [==============================] - 0s 162us/sample - loss: 0.2939 - accuracy: 0.9186 - val_loss: 0.2903 - val_accuracy: 0.9202\n",
      "Epoch 24/100\n",
      "381/381 [==============================] - 0s 144us/sample - loss: 0.2848 - accuracy: 0.9239 - val_loss: 0.2821 - val_accuracy: 0.9202\n",
      "Epoch 25/100\n",
      "381/381 [==============================] - 0s 159us/sample - loss: 0.2764 - accuracy: 0.9265 - val_loss: 0.2746 - val_accuracy: 0.9255\n",
      "Epoch 26/100\n",
      "381/381 [==============================] - 0s 147us/sample - loss: 0.2684 - accuracy: 0.9265 - val_loss: 0.2675 - val_accuracy: 0.9255\n",
      "Epoch 27/100\n",
      "381/381 [==============================] - 0s 152us/sample - loss: 0.2611 - accuracy: 0.9291 - val_loss: 0.2609 - val_accuracy: 0.9255\n",
      "Epoch 28/100\n",
      "381/381 [==============================] - 0s 145us/sample - loss: 0.2540 - accuracy: 0.9370 - val_loss: 0.2547 - val_accuracy: 0.9255\n",
      "Epoch 29/100\n",
      "381/381 [==============================] - 0s 159us/sample - loss: 0.2475 - accuracy: 0.9396 - val_loss: 0.2488 - val_accuracy: 0.9415\n",
      "Epoch 30/100\n",
      "381/381 [==============================] - 0s 147us/sample - loss: 0.2412 - accuracy: 0.9396 - val_loss: 0.2434 - val_accuracy: 0.9415\n",
      "Epoch 31/100\n",
      "381/381 [==============================] - 0s 167us/sample - loss: 0.2354 - accuracy: 0.9396 - val_loss: 0.2383 - val_accuracy: 0.9415\n",
      "Epoch 32/100\n",
      "381/381 [==============================] - 0s 172us/sample - loss: 0.2298 - accuracy: 0.9396 - val_loss: 0.2334 - val_accuracy: 0.9415\n",
      "Epoch 33/100\n",
      "381/381 [==============================] - 0s 176us/sample - loss: 0.2245 - accuracy: 0.9449 - val_loss: 0.2290 - val_accuracy: 0.9415\n",
      "Epoch 34/100\n",
      "381/381 [==============================] - 0s 165us/sample - loss: 0.2195 - accuracy: 0.9449 - val_loss: 0.2247 - val_accuracy: 0.9415\n",
      "Epoch 35/100\n",
      "381/381 [==============================] - 0s 155us/sample - loss: 0.2148 - accuracy: 0.9475 - val_loss: 0.2206 - val_accuracy: 0.9415\n",
      "Epoch 36/100\n",
      "381/381 [==============================] - 0s 158us/sample - loss: 0.2103 - accuracy: 0.9501 - val_loss: 0.2167 - val_accuracy: 0.9415\n",
      "Epoch 37/100\n",
      "381/381 [==============================] - 0s 168us/sample - loss: 0.2060 - accuracy: 0.9528 - val_loss: 0.2130 - val_accuracy: 0.9415\n",
      "Epoch 38/100\n",
      "381/381 [==============================] - 0s 152us/sample - loss: 0.2018 - accuracy: 0.9554 - val_loss: 0.2096 - val_accuracy: 0.9415\n",
      "Epoch 39/100\n",
      "381/381 [==============================] - 0s 153us/sample - loss: 0.1980 - accuracy: 0.9554 - val_loss: 0.2062 - val_accuracy: 0.9415\n",
      "Epoch 40/100\n",
      "381/381 [==============================] - 0s 146us/sample - loss: 0.1942 - accuracy: 0.9554 - val_loss: 0.2030 - val_accuracy: 0.9468\n",
      "Epoch 41/100\n",
      "381/381 [==============================] - 0s 145us/sample - loss: 0.1906 - accuracy: 0.9554 - val_loss: 0.2000 - val_accuracy: 0.9468\n",
      "Epoch 42/100\n",
      "381/381 [==============================] - 0s 158us/sample - loss: 0.1873 - accuracy: 0.9554 - val_loss: 0.1971 - val_accuracy: 0.9468\n",
      "Epoch 43/100\n",
      "381/381 [==============================] - 0s 142us/sample - loss: 0.1840 - accuracy: 0.9554 - val_loss: 0.1944 - val_accuracy: 0.9468\n",
      "Epoch 44/100\n",
      "381/381 [==============================] - 0s 158us/sample - loss: 0.1808 - accuracy: 0.9606 - val_loss: 0.1917 - val_accuracy: 0.9415\n",
      "Epoch 45/100\n",
      "381/381 [==============================] - 0s 152us/sample - loss: 0.1779 - accuracy: 0.9606 - val_loss: 0.1892 - val_accuracy: 0.9362\n",
      "Epoch 46/100\n",
      "381/381 [==============================] - 0s 164us/sample - loss: 0.1750 - accuracy: 0.9606 - val_loss: 0.1868 - val_accuracy: 0.9362\n",
      "Epoch 47/100\n",
      "381/381 [==============================] - 0s 148us/sample - loss: 0.1722 - accuracy: 0.9606 - val_loss: 0.1846 - val_accuracy: 0.9362\n",
      "Epoch 48/100\n",
      "381/381 [==============================] - 0s 155us/sample - loss: 0.1695 - accuracy: 0.9606 - val_loss: 0.1824 - val_accuracy: 0.9362\n",
      "Epoch 49/100\n",
      "381/381 [==============================] - 0s 148us/sample - loss: 0.1670 - accuracy: 0.9606 - val_loss: 0.1802 - val_accuracy: 0.9362\n",
      "Epoch 50/100\n",
      "381/381 [==============================] - 0s 155us/sample - loss: 0.1645 - accuracy: 0.9606 - val_loss: 0.1782 - val_accuracy: 0.9362\n",
      "Epoch 51/100\n",
      "381/381 [==============================] - 0s 153us/sample - loss: 0.1621 - accuracy: 0.9606 - val_loss: 0.1762 - val_accuracy: 0.9362\n",
      "Epoch 52/100\n",
      "381/381 [==============================] - 0s 151us/sample - loss: 0.1598 - accuracy: 0.9606 - val_loss: 0.1743 - val_accuracy: 0.9362\n",
      "Epoch 53/100\n",
      "381/381 [==============================] - 0s 153us/sample - loss: 0.1576 - accuracy: 0.9606 - val_loss: 0.1725 - val_accuracy: 0.9362\n",
      "Epoch 54/100\n",
      "381/381 [==============================] - 0s 151us/sample - loss: 0.1555 - accuracy: 0.9606 - val_loss: 0.1707 - val_accuracy: 0.9362\n",
      "Epoch 55/100\n",
      "381/381 [==============================] - 0s 152us/sample - loss: 0.1534 - accuracy: 0.9606 - val_loss: 0.1691 - val_accuracy: 0.9362\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381/381 [==============================] - 0s 151us/sample - loss: 0.1514 - accuracy: 0.9606 - val_loss: 0.1675 - val_accuracy: 0.9362\n",
      "Epoch 57/100\n",
      "381/381 [==============================] - 0s 144us/sample - loss: 0.1495 - accuracy: 0.9606 - val_loss: 0.1660 - val_accuracy: 0.9362\n",
      "Epoch 58/100\n",
      "381/381 [==============================] - 0s 148us/sample - loss: 0.1476 - accuracy: 0.9606 - val_loss: 0.1644 - val_accuracy: 0.9415\n",
      "Epoch 59/100\n",
      "381/381 [==============================] - 0s 140us/sample - loss: 0.1458 - accuracy: 0.9606 - val_loss: 0.1630 - val_accuracy: 0.9415\n",
      "Epoch 60/100\n",
      "381/381 [==============================] - 0s 160us/sample - loss: 0.1440 - accuracy: 0.9606 - val_loss: 0.1616 - val_accuracy: 0.9415\n",
      "Epoch 61/100\n",
      "381/381 [==============================] - 0s 157us/sample - loss: 0.1423 - accuracy: 0.9606 - val_loss: 0.1602 - val_accuracy: 0.9415\n",
      "Epoch 62/100\n",
      "381/381 [==============================] - 0s 144us/sample - loss: 0.1407 - accuracy: 0.9606 - val_loss: 0.1589 - val_accuracy: 0.9415\n",
      "Epoch 63/100\n",
      "381/381 [==============================] - 0s 142us/sample - loss: 0.1391 - accuracy: 0.9606 - val_loss: 0.1576 - val_accuracy: 0.9415\n",
      "Epoch 64/100\n",
      "381/381 [==============================] - 0s 142us/sample - loss: 0.1375 - accuracy: 0.9606 - val_loss: 0.1564 - val_accuracy: 0.9415\n",
      "Epoch 65/100\n",
      "381/381 [==============================] - 0s 139us/sample - loss: 0.1361 - accuracy: 0.9606 - val_loss: 0.1552 - val_accuracy: 0.9468\n",
      "Epoch 66/100\n",
      "381/381 [==============================] - 0s 140us/sample - loss: 0.1345 - accuracy: 0.9606 - val_loss: 0.1541 - val_accuracy: 0.9468\n",
      "Epoch 67/100\n",
      "381/381 [==============================] - 0s 140us/sample - loss: 0.1331 - accuracy: 0.9606 - val_loss: 0.1529 - val_accuracy: 0.9468\n",
      "Epoch 68/100\n",
      "381/381 [==============================] - 0s 140us/sample - loss: 0.1318 - accuracy: 0.9606 - val_loss: 0.1518 - val_accuracy: 0.9468\n",
      "Epoch 69/100\n",
      "381/381 [==============================] - 0s 142us/sample - loss: 0.1304 - accuracy: 0.9633 - val_loss: 0.1508 - val_accuracy: 0.9468\n",
      "Epoch 70/100\n",
      "381/381 [==============================] - 0s 146us/sample - loss: 0.1291 - accuracy: 0.9633 - val_loss: 0.1497 - val_accuracy: 0.9468\n",
      "Epoch 71/100\n",
      "381/381 [==============================] - 0s 143us/sample - loss: 0.1278 - accuracy: 0.9633 - val_loss: 0.1487 - val_accuracy: 0.9468\n",
      "Epoch 72/100\n",
      "381/381 [==============================] - 0s 143us/sample - loss: 0.1266 - accuracy: 0.9633 - val_loss: 0.1477 - val_accuracy: 0.9468\n",
      "Epoch 73/100\n",
      "381/381 [==============================] - 0s 137us/sample - loss: 0.1254 - accuracy: 0.9633 - val_loss: 0.1468 - val_accuracy: 0.9468\n",
      "Epoch 74/100\n",
      "381/381 [==============================] - 0s 142us/sample - loss: 0.1241 - accuracy: 0.9633 - val_loss: 0.1459 - val_accuracy: 0.9468\n",
      "Epoch 75/100\n",
      "381/381 [==============================] - 0s 141us/sample - loss: 0.1230 - accuracy: 0.9633 - val_loss: 0.1450 - val_accuracy: 0.9468\n",
      "Epoch 76/100\n",
      "381/381 [==============================] - 0s 165us/sample - loss: 0.1219 - accuracy: 0.9633 - val_loss: 0.1441 - val_accuracy: 0.9468\n",
      "Epoch 77/100\n",
      "381/381 [==============================] - 0s 146us/sample - loss: 0.1208 - accuracy: 0.9659 - val_loss: 0.1433 - val_accuracy: 0.9468\n",
      "Epoch 78/100\n",
      "381/381 [==============================] - 0s 140us/sample - loss: 0.1197 - accuracy: 0.9659 - val_loss: 0.1425 - val_accuracy: 0.9468\n",
      "Epoch 79/100\n",
      "381/381 [==============================] - 0s 140us/sample - loss: 0.1187 - accuracy: 0.9659 - val_loss: 0.1417 - val_accuracy: 0.9468\n",
      "Epoch 80/100\n",
      "381/381 [==============================] - 0s 142us/sample - loss: 0.1176 - accuracy: 0.9659 - val_loss: 0.1409 - val_accuracy: 0.9468\n",
      "Epoch 81/100\n",
      "381/381 [==============================] - 0s 139us/sample - loss: 0.1166 - accuracy: 0.9659 - val_loss: 0.1402 - val_accuracy: 0.9468\n",
      "Epoch 82/100\n",
      "381/381 [==============================] - 0s 138us/sample - loss: 0.1157 - accuracy: 0.9659 - val_loss: 0.1394 - val_accuracy: 0.9468\n",
      "Epoch 83/100\n",
      "381/381 [==============================] - 0s 149us/sample - loss: 0.1147 - accuracy: 0.9685 - val_loss: 0.1387 - val_accuracy: 0.9468\n",
      "Epoch 84/100\n",
      "381/381 [==============================] - 0s 140us/sample - loss: 0.1138 - accuracy: 0.9685 - val_loss: 0.1380 - val_accuracy: 0.9468\n",
      "Epoch 85/100\n",
      "381/381 [==============================] - 0s 141us/sample - loss: 0.1129 - accuracy: 0.9685 - val_loss: 0.1373 - val_accuracy: 0.9468\n",
      "Epoch 86/100\n",
      "381/381 [==============================] - 0s 139us/sample - loss: 0.1120 - accuracy: 0.9685 - val_loss: 0.1366 - val_accuracy: 0.9468\n",
      "Epoch 87/100\n",
      "381/381 [==============================] - 0s 138us/sample - loss: 0.1111 - accuracy: 0.9685 - val_loss: 0.1360 - val_accuracy: 0.9468\n",
      "Epoch 88/100\n",
      "381/381 [==============================] - 0s 142us/sample - loss: 0.1102 - accuracy: 0.9685 - val_loss: 0.1354 - val_accuracy: 0.9468\n",
      "Epoch 89/100\n",
      "381/381 [==============================] - 0s 137us/sample - loss: 0.1094 - accuracy: 0.9685 - val_loss: 0.1347 - val_accuracy: 0.9521\n",
      "Epoch 90/100\n",
      "381/381 [==============================] - 0s 139us/sample - loss: 0.1085 - accuracy: 0.9685 - val_loss: 0.1341 - val_accuracy: 0.9521\n",
      "Epoch 91/100\n",
      "381/381 [==============================] - 0s 144us/sample - loss: 0.1077 - accuracy: 0.9685 - val_loss: 0.1335 - val_accuracy: 0.9521\n",
      "Epoch 92/100\n",
      "381/381 [==============================] - 0s 141us/sample - loss: 0.1069 - accuracy: 0.9685 - val_loss: 0.1329 - val_accuracy: 0.9521\n",
      "Epoch 93/100\n",
      "381/381 [==============================] - 0s 136us/sample - loss: 0.1062 - accuracy: 0.9685 - val_loss: 0.1324 - val_accuracy: 0.9521\n",
      "Epoch 94/100\n",
      "381/381 [==============================] - 0s 136us/sample - loss: 0.1054 - accuracy: 0.9738 - val_loss: 0.1318 - val_accuracy: 0.9521\n",
      "Epoch 95/100\n",
      "381/381 [==============================] - 0s 142us/sample - loss: 0.1046 - accuracy: 0.9738 - val_loss: 0.1312 - val_accuracy: 0.9521\n",
      "Epoch 96/100\n",
      "381/381 [==============================] - 0s 140us/sample - loss: 0.1039 - accuracy: 0.9738 - val_loss: 0.1307 - val_accuracy: 0.9521\n",
      "Epoch 97/100\n",
      "381/381 [==============================] - 0s 148us/sample - loss: 0.1032 - accuracy: 0.9738 - val_loss: 0.1302 - val_accuracy: 0.9521\n",
      "Epoch 98/100\n",
      "381/381 [==============================] - 0s 137us/sample - loss: 0.1025 - accuracy: 0.9738 - val_loss: 0.1297 - val_accuracy: 0.9521\n",
      "Epoch 99/100\n",
      "381/381 [==============================] - 0s 138us/sample - loss: 0.1018 - accuracy: 0.9738 - val_loss: 0.1292 - val_accuracy: 0.9521\n",
      "Epoch 100/100\n",
      "381/381 [==============================] - 0s 143us/sample - loss: 0.1011 - accuracy: 0.9738 - val_loss: 0.1287 - val_accuracy: 0.9521\n"
     ]
    }
   ],
   "source": [
    "r= model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "381/1 [======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 32us/sample - loss: 0.0972 - accuracy: 0.9764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score [0.10071514638781234, 0.97637796]\n",
      "188/1 [========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 35us/sample - loss: 0.1932 - accuracy: 0.9521\n",
      "Test score [0.1286885586190731, 0.95212764]\n"
     ]
    }
   ],
   "source": [
    "# train and test score\n",
    "print(\"Train score\",model.evaluate(X_train,Y_train))\n",
    "print(\"Test score\",model.evaluate(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(r.history['loss'],label='loss')\n",
    "plt.plot(r.history['val_loss'],label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=scaler.fit_transform(x)\n",
    "cl = pd.DataFrame(model.predict_classes(x),columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class']=cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([data['id'],data['class']]).T.to_csv('Submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
